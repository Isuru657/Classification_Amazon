{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (0.23.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.8/site-packages (1.19.2)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: hyperopt in /opt/anaconda3/lib/python3.8/site-packages (0.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.8/site-packages (from hyperopt) (1.6.0)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.8/site-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/anaconda3/lib/python3.8/site-packages (from hyperopt) (2.5)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from hyperopt) (1.15.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from networkx>=2.2->hyperopt) (4.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn numpy nltk hyperopt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "handle_text: takes out stop words and punctuation, lemmatizes, and converts to lowercase\n",
    "* input: string\n",
    "* output: hanlded string\n",
    "\"\"\"\n",
    "def handle_text(text):\n",
    "    # Check that argument provided is a string\n",
    "    if not isinstance(text,str):\n",
    "        return \"\"\n",
    "    output = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for word in nltk.word_tokenize(text): \n",
    "        # Disclude stop words and words with punctuation\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            # Add lemmatized words\n",
    "            output.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return \" \".join(output) # return the list of words as one single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "convert_text: applies handle_text to all summaries and reviews in dataframe\n",
    "* input: dataframe\n",
    "* output: new dataframe with converted summaries/reviews\n",
    "\"\"\"\n",
    "def convert_text(df):\n",
    "    df = df.assign(summary=df['summary'].apply(handle_text))\n",
    "    df = df.assign(reviewText=df['reviewText'].apply(handle_text))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_uncommon_words: gets all words that occur less than the 'max' times in the data\n",
    "* inputs: list_of_strings to get the uncommon words from\n",
    "         max: integer of the max number of times a word can occur and still be considered 'uncommon'\n",
    "* output: dictionary key set of uncommon words\n",
    "\"\"\"\n",
    "def get_uncommon_words(list_of_strings, max):\n",
    "    uncommon_words = {} # dictionary of uncommon words where key=word, item=frequency\n",
    "    seen = {\"\"} # set of 'seen' words to avoid re-adding words to uncommon_words\n",
    "    for string in list_of_strings:\n",
    "        for word in nltk.word_tokenize(string): \n",
    "            count = uncommon_words.get(word)\n",
    "            # If we haven't seen the word yet, add it to seen and uncommon_words\n",
    "            if word not in seen: \n",
    "                seen.add(word)\n",
    "                uncommon_words.update({word: 1})\n",
    "            # If it's already been seen the max number of times, take it out of uncommon_words\n",
    "            elif count == max: \n",
    "                uncommon_words.pop(word)\n",
    "            # If it hasn't been seen max times, increment its count in uncommon_words\n",
    "            elif count is not None:\n",
    "                uncommon_words.update({word: count+1})\n",
    "    return uncommon_words.keys() # return only the words, not the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "remove_words: removes all words in provided list from a given string\n",
    "* inputs: string (to remove words from)\n",
    "          words_to_remove: list of strings indicating which words to remove\n",
    "* output: string (without provided words)\n",
    "\"\"\"\n",
    "def remove_words(string, words_to_remove):\n",
    "    list_of_words = nltk.word_tokenize(string)\n",
    "    for word in list_of_words: \n",
    "        # remove all words that occur in words_to_remove\n",
    "        if word in words_to_remove: \n",
    "            list_of_words.remove(word)\n",
    "    return \" \".join(list_of_words) # return as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "remove_words_summaries: removes all words in provided list from the dataframe's summaries\n",
    "* inputs: dataframe (to remove words from)\n",
    "          words_to_remove: list of strings indicating which words to remove\n",
    "* output: new dataframe (without the given words)\n",
    "\"\"\"\n",
    "def remove_words_reviews(df, words_to_remove):\n",
    "    return df.assign(reviewText=df['reviewText'].apply(remove_words, words_to_remove=words_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "combine_summaries: gets a list where each item corresponds to all the summaries for one product\n",
    "* input: dataframe\n",
    "* output: list of product summaries\n",
    "\"\"\"\n",
    "def combine_summaries(train_df, test_df):\n",
    "#    df, vestorizer\n",
    "    #aggregate reviewTest based on amazon-id\n",
    "    combined_train_df = train_df.groupby('amazon-id').agg({'reviewText': ' '.join, 'summary': ' '.join})\n",
    "    vectorizer = TfidfVectorizer(max_features=8000)\n",
    "    \n",
    "    #transform it into an np.array then fit it into matrix\n",
    "    review_vector = np.array(vectorizer.fit_transform(combined_train_df['reviewText']).toarray())\n",
    "    #saved_train_matrix = vectorizer.fit(combined_train_df['reviewText'])\n",
    "    summary_vector = np.array(vectorizer.transform(combined_train_df['summary']).toarray())\n",
    "    \n",
    "    \n",
    "    train_vector = review_vector + summary_vector\n",
    "    #train_vector = np.array(vectorizer.fit_transform(combined_train_df['reviewText']).toarray())\n",
    "    \n",
    "    combined_test_df = test_df.groupby('amazon-id').agg({'reviewText': ' '.join, 'summary': ' '.join})\n",
    "\n",
    "    #transform it into an np.array then fit it into matrix\n",
    "    review_vector = np.array(vectorizer.fit_transform(combined_test_df['reviewText']).toarray())\n",
    "    #saved_test_matrix = vectorizer.fit(combined_test_df['reviewText'])\n",
    "    summary_test_vector = np.array(vectorizer.transform(combined_test_df['summary']).toarray())\n",
    "\n",
    "    #combine review and summary\n",
    "    test_vector = np.array(vectorizer.fit_transform(combined_test_df['reviewText']).toarray())\n",
    "    print(\"Test vector length: \", len(test_vector))\n",
    "    print(\"Train vector length: \", len(train_vector))\n",
    "    return train_vector, test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "awesome_products: gets a list of whether each product is awesome or not\n",
    "* input: dataframe\n",
    "* output: list of whether product is awesome or not \n",
    "\"\"\"\n",
    "def awesome_products(df):\n",
    "    df = df.groupby('amazon-id').agg({'overall': lambda x: 1 if np.mean(x) > 4.5 else 0})\n",
    "    return df['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "prep_df: gets dataframe ready for predictions by handling the summary and review text \n",
    "* input: dataframe\n",
    "* output: handled dataframe\n",
    "\"\"\"\n",
    "def prep_df(df):\n",
    "    # Convert to lowercase, remove punctuation and stop words, lemmatize words\n",
    "    df = convert_text(df)\n",
    "    uncommon_words = get_uncommon_words(df['reviewText'], 2)\n",
    "     # Remove uncommon words\n",
    "    return remove_words_reviews(df, uncommon_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make_prediction: makes prection based on training and test data\n",
    "* inputs: x_train, y_train - training data, used to make the model\n",
    "          x_test - test data to make prediction from\n",
    "* output: predicted y values based on test data\n",
    "\n",
    "make_prediction separates the model-making from the predicting in order to save space\n",
    "\n",
    "\"\"\"\n",
    "def make_prediction(x_train, y_train, x_test):\n",
    "    model = LinearSVC()\n",
    "    model.fit(x_train, y_train)\n",
    "    return model, model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test_model(): runs the model on training data and prints f1-score and classification report\n",
    "\"\"\"\n",
    "def test_model():\n",
    "    df = pd.read_csv('datasets/Train.csv')\n",
    "    df = prep_df(df)\n",
    "    train_vector, test_vector= combine_summaries(df, df)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_vector, awesome_products(df))\n",
    "    model, y_pred = make_prediction(x_train, y_train, x_test)\n",
    "    \n",
    "    print(\"f1_score=\", f1_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return x_train, x_test, y_train, y_test, model, train_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test vector length:  10543\n",
      "Train vector shape:  10543\n",
      "Awesome length:  amazon-id\n",
      "-9217723718720870868    0\n",
      "-9215746463819797371    1\n",
      "-9213978596308513604    0\n",
      "-9211290576571923870    0\n",
      "-9208769561690910545    0\n",
      "-9208198150317071838    1\n",
      "-9207623668807116759    1\n",
      "-9203846742259231188    0\n",
      "-9202734100323412002    1\n",
      "-9198322104429679287    0\n",
      "-9196816809534721621    1\n",
      "-9195787036214670721    0\n",
      "-9195715895034041804    0\n",
      "-9185450485401072051    0\n",
      "-9185028087385392712    1\n",
      "-9184754614868031288    0\n",
      "-9181938828093623181    0\n",
      "-9177871146610584170    0\n",
      "-9175284111494784505    1\n",
      "-9171391896663756617    1\n",
      "-9171133252298665868    0\n",
      "-9170624073749960483    0\n",
      "-9168022306965593241    0\n",
      "-9164485698573154140    1\n",
      "-9163887239523687605    1\n",
      "-9163211980141980025    1\n",
      "-9163010614846223784    0\n",
      "-9162759304190308282    0\n",
      "-9161926972130019940    0\n",
      "-9157117359304219671    1\n",
      "                       ..\n",
      " 9182231784957840868    0\n",
      " 9183110027749334191    0\n",
      " 9183152728540863858    0\n",
      " 9183587774518525868    1\n",
      " 9184679746257273506    1\n",
      " 9187454525560420789    1\n",
      " 9191699717616638912    1\n",
      " 9192303539297499270    1\n",
      " 9193396673702558183    1\n",
      " 9194212252781986285    1\n",
      " 9195028134778603244    1\n",
      " 9199259431832163867    1\n",
      " 9200528860526857499    1\n",
      " 9202286158489947905    0\n",
      " 9205628540934353815    1\n",
      " 9206943141300942065    1\n",
      " 9207473202971945942    1\n",
      " 9208467423485772129    0\n",
      " 9211171646322169604    1\n",
      " 9211338909954962026    1\n",
      " 9211922862265284272    0\n",
      " 9212043675364570807    1\n",
      " 9213747440163065679    1\n",
      " 9216098654890788813    0\n",
      " 9217670041811029322    0\n",
      " 9218870320655141661    0\n",
      " 9221578337502519209    1\n",
      " 9221615570697142155    1\n",
      " 9221801008952598876    0\n",
      " 9222652928856141170    0\n",
      "Name: overall, Length: 10543, dtype: int64\n",
      "f1_score= 0.7412140575079873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.60      0.62      1112\n",
      "           1       0.72      0.76      0.74      1524\n",
      "\n",
      "    accuracy                           0.69      2636\n",
      "   macro avg       0.68      0.68      0.68      2636\n",
      "weighted avg       0.69      0.69      0.69      2636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, model, train_vector = test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "run_model(): runs the model on testing data and outputs predictions.csv file\n",
    "\"\"\"\n",
    "def run_model(): \n",
    "    df_train = pd.read_csv('datasets/Train.csv')\n",
    "    print(\"Prepping train df\")\n",
    "    df_train = prep_df(df_train)\n",
    "    print(\"Done\")\n",
    "    \n",
    "#   x_train = combine_summaries(df_train)\n",
    "    print(\"getting y train\")\n",
    "    y_train = awesome_products(df_train)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    \n",
    "    df_test = pd.read_csv('datasets/Test.csv')\n",
    "    print(\"Prepping test df\")\n",
    "    test_df = prep_df(df_test)\n",
    "    print(\"Done\")\n",
    "    #x_test = combine_summaries(df_test)\n",
    "    \n",
    "    print(\"Combining summaries\")\n",
    "    x_train, x_test = combine_summaries(df_train, test_df)\n",
    "    #print(\"x_train\")\n",
    "    #print(x_train)\n",
    "    #print(\"x_test\")\n",
    "    #print(len(x_test))\n",
    "    #print(\"y_train\")\n",
    "    print(len(y_train))\n",
    "    print(\"Done\")\n",
    "    \n",
    "    print(\"Making prediction\")\n",
    "    y_pred = make_prediction(x_train, y_train, x_test)\n",
    "    print(\"y_pred\")\n",
    "    print(len(y_pred))\n",
    "    print(\"Done\")\n",
    "    #output = pd.DataFrame({'amazon-id': df_test[\"amazon-id\"].drop_duplicates(), 'Awesome': y_pred})\n",
    "    \n",
    "    print(\"Writing output\")\n",
    "    output = pd.DataFrame({'amazon-id': df_test[\"amazon-id\"].drop_duplicates().reset_index(drop=True), 'Awesome': y_pred})\n",
    "    output.to_csv('predictions.csv')\n",
    "    print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper param tuning\n",
    "\n",
    "loss = ['hinge', 'squared_hinge']\n",
    "cs = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 1]\n",
    "tol = [0.00005, 0.00007, 0.0001]\n",
    "param_grid = {'loss': loss, 'C': cs, 'tol': tol}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_SVC_class = RandomizedSearchCV(\n",
    "    estimator = model,\n",
    "    param_distributions = param_grid,\n",
    "    n_iter = 10,\n",
    "    scoring='accuracy', n_jobs=4, cv = 3, refit=True, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=0.04, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=5e-05,\n",
      "          verbose=0)\n",
      "0.7163273049196914\n"
     ]
    }
   ],
   "source": [
    "random_SVC_class.fit(x_train, y_train)\n",
    "print(random_SVC_class.best_estimator_)\n",
    "print(random_SVC_class.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_SVC_class = GridSearchCV(\n",
    "    estimator= model,\n",
    "    param_grid= param_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=4,\n",
    "    cv=5,\n",
    "    refit= True, return_train_score= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=0.05, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=5e-05,\n",
      "          verbose=0)\n",
      "0.7266978626533451\n"
     ]
    }
   ],
   "source": [
    "grid_SVC_class.fit(x_train, y_train)\n",
    "print(grid_SVC_class.best_estimator_)\n",
    "print(grid_SVC_class.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping train df\n",
      "Done\n",
      "getting y train\n",
      "Awesome length:  amazon-id\n",
      "-9217723718720870868    0\n",
      "-9215746463819797371    1\n",
      "-9213978596308513604    0\n",
      "-9211290576571923870    0\n",
      "-9208769561690910545    0\n",
      "-9208198150317071838    1\n",
      "-9207623668807116759    1\n",
      "-9203846742259231188    0\n",
      "-9202734100323412002    1\n",
      "-9198322104429679287    0\n",
      "-9196816809534721621    1\n",
      "-9195787036214670721    0\n",
      "-9195715895034041804    0\n",
      "-9185450485401072051    0\n",
      "-9185028087385392712    1\n",
      "-9184754614868031288    0\n",
      "-9181938828093623181    0\n",
      "-9177871146610584170    0\n",
      "-9175284111494784505    1\n",
      "-9171391896663756617    1\n",
      "-9171133252298665868    0\n",
      "-9170624073749960483    0\n",
      "-9168022306965593241    0\n",
      "-9164485698573154140    1\n",
      "-9163887239523687605    1\n",
      "-9163211980141980025    1\n",
      "-9163010614846223784    0\n",
      "-9162759304190308282    0\n",
      "-9161926972130019940    0\n",
      "-9157117359304219671    1\n",
      "                       ..\n",
      " 9182231784957840868    0\n",
      " 9183110027749334191    0\n",
      " 9183152728540863858    0\n",
      " 9183587774518525868    1\n",
      " 9184679746257273506    1\n",
      " 9187454525560420789    1\n",
      " 9191699717616638912    1\n",
      " 9192303539297499270    1\n",
      " 9193396673702558183    1\n",
      " 9194212252781986285    1\n",
      " 9195028134778603244    1\n",
      " 9199259431832163867    1\n",
      " 9200528860526857499    1\n",
      " 9202286158489947905    0\n",
      " 9205628540934353815    1\n",
      " 9206943141300942065    1\n",
      " 9207473202971945942    1\n",
      " 9208467423485772129    0\n",
      " 9211171646322169604    1\n",
      " 9211338909954962026    1\n",
      " 9211922862265284272    0\n",
      " 9212043675364570807    1\n",
      " 9213747440163065679    1\n",
      " 9216098654890788813    0\n",
      " 9217670041811029322    0\n",
      " 9218870320655141661    0\n",
      " 9221578337502519209    1\n",
      " 9221615570697142155    1\n",
      " 9221801008952598876    0\n",
      " 9222652928856141170    0\n",
      "Name: overall, Length: 10543, dtype: int64\n",
      "Done\n",
      "Prepping test df\n",
      "Done\n",
      "Combining summaries\n",
      "Test vector length:  1172\n",
      "Train vector shape:  10543\n",
      "Done\n",
      "Making prediction\n",
      "y_pred\n",
      "2\n",
      "Done\n",
      "Writing output\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array length 2 does not match index length 1172",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-d8af0fd933a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-2cd6c554c865>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Writing output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'amazon-id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"amazon-id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Awesome'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predictions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    326\u001b[0m                            \u001b[0;34m'length {idx_len}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                            .format(length=lengths[0], idx_len=len(index)))\n\u001b[0;32m--> 328\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mibase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array length 2 does not match index length 1172"
     ]
    }
   ],
   "source": [
    "run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL, ATTEMPTED, AND FAILED CODES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation:\n",
    "\n",
    "We performed k fold cross validation on the model. However, we were unsure about the ideal k value for cross validation so we had another method to compute the k value that yields the highest accuracy between a specified range.\n",
    "\n",
    "Unfortunately, both methods only worked on smaller subsets of the data, and crashed when the method was called on the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "evaluate_model: performs k fold cross validation  \n",
    "* input: x_train, x_test, y_test, model, cv\n",
    "* output: the mean of running cross validation across 'cv' folds         \n",
    "\"\"\"\n",
    "\n",
    "def evaluate_model(x_train, x_test, y_test, model, cv):\n",
    "#     matrix, saved_matrix = word_matrix(x_train)\n",
    "#     X = saved_matrix.transform(x_test).toarray()\n",
    "    scores = cross_val_score(model, X, y_test, scoring=\"accuracy\", cv= cv)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "compare_to_ideal: computes the best k value for k fold cross validation within a given range\n",
    "* input: minK: minimum k value in a range\n",
    "         maxK: maximum k value in a range\n",
    "         x_test\n",
    "         x_train\n",
    "         y_test\n",
    "* output: dev_means: dictionary of the means of cross validation of different k values\n",
    "          the k value with the highest accuracy\n",
    "\"\"\"\n",
    "def compare_to_idealK(minK, maxK, x_test, x_train, y_test, model):\n",
    "     #ideal_mean =  evaluate_model(x_train, x_test, y_test, model, LeaveOneOut())\n",
    "    folds= range(minK, maxK)\n",
    "    dev_means={}\n",
    "    \n",
    "    for k in folds:\n",
    "        cv = KFold(n_splits=k, shuffle= True, random_state=1)\n",
    "        k_mean= evaluate_model(x_train, x_test, y_test, model, cv)\n",
    "        dev_means[k] = k_mean #abs(ideal_mean - k_mean)\n",
    "        minK= minK+1\n",
    "    \n",
    "    return dev_means, max(dev_means, key=dev_means.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer and GaussianNB approach: our first attempt at predicting from summary data-- we ultimately changed to TFIDF and LinearSVC because this approach was only getting us F1-scores of around .4-.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "word_matrix: converts a list of words to a matrix of 0s and 1s \n",
    "* input: list_of_words to convert\n",
    "* outputs: array of 0s and 1s of words in list_of_words\n",
    "           saved matrix: saves how we fit training data to the model, will be\n",
    "                         used to transform the test data to fit the model\n",
    "\"\"\"\n",
    "def word_matrix(list_of_words): \n",
    "    vectorizer = CountVectorizer()\n",
    "    # Convert words to matrix form\n",
    "    matrix = vectorizer.fit_transform(list_of_words)\n",
    "    # Save the matrix to later transform the test data\n",
    "    saved_matrix = vectorizer.fit(list_of_words)\n",
    "    return matrix.toarray(), saved_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make_gaussiannb_model: makes a gaussian naive bayes model using the training data\n",
    "* inputs: x_train, y_train\n",
    "* output: GaussianNB model\n",
    "\"\"\"\n",
    "def make_gaussiannb_model(x_train, y_train): \n",
    "    model = GaussianNB()\n",
    "    num_rows, num_cols = x_train.shape\n",
    "    i=0\n",
    "    # Use partial_fit to save space \n",
    "    # Add 1000 rows of the training data into the model at a time\n",
    "    while i+1000<=num_rows: \n",
    "        model.partial_fit(x_train[i:i+1000],y_train[i:i+1000], classes=[0,1])\n",
    "        i+=1000\n",
    "    # Fit the remaining data\n",
    "    model.partial_fit(x_train[i:num_rows], y_train[i:num_rows], classes=[0,1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "predict: makes prediction based on provided model and test data\n",
    "* inputs: model: GaussianNB model from training data\n",
    "          saved_matrix: to transform the test data to fit the model\n",
    "* output: predicted y values based on test data\n",
    "\"\"\"\n",
    "def predict(model, saved_matrix, x_test):\n",
    "    X = saved_matrix.transform(x_test).toarray()\n",
    "    return model, model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_model: gets the GaussianNB model from training data \n",
    "* inputs: x_train, y_train\n",
    "* outputs: model: GaussianNB model from training data\n",
    "           saved matrix: saves how we fit training data to the model, will be\n",
    "                         used to transform the test data to fit the model\n",
    "\"\"\"\n",
    "def get_model(x_train, y_train):\n",
    "    matrix, saved_matrix = word_matrix(x_train)\n",
    "    print(matrix)\n",
    "    model = make_gaussiannb_model(matrix, y_train)\n",
    "    return model, saved_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories: \n",
    "\n",
    "Creating a dictionary of average star rating based for each category genre on training data. Then using this dictionary to predict star review for a product by looking at it's category. \n",
    "\n",
    "We ultimately did not end up using this because we felt using aggregation of star values for a single categorical genre wouldn't accurately give a great prediction, especially when it is not working in conjunction with more heavily influencial feature such as the Summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates star rating for each category based on training data\n",
    "\n",
    "def category_rating(df):\n",
    "    categories_genre = df['categories']\n",
    "    overall_points = df['overall']\n",
    "\n",
    "    dictionary={}\n",
    "    list_splits = []\n",
    "\n",
    "    index =0\n",
    "    index_split = 0\n",
    "\n",
    "    #going through each categories\n",
    "    while index < len(df.index):\n",
    "\n",
    "        #within each categories, there are different genres\n",
    "        list_splits = categories_genre[index].split(\",\")\n",
    "\n",
    "        #splittings genre within categories, loop through each genre \n",
    "        while index_split < len(list_splits):\n",
    "\n",
    "            #strip to make it look cleaner\n",
    "            list_splits[index_split]= list_splits[index_split].strip(\"]\")\n",
    "            list_splits[index_split]= list_splits[index_split].strip(\"[\")\n",
    "            list_splits[index_split]= list_splits[index_split].strip()\n",
    "\n",
    "            #if genre is not within the category, add it to the dictionary\n",
    "            if list_splits[index_split] not in dictionary.keys():\n",
    "                dictionary[list_splits[index_split]] = overall_points[index]\n",
    "\n",
    "            #if genre is within the category, add the value with the old value and average it\n",
    "            else:\n",
    "                dictionary[list_splits[index_split]] = (dictionary[list_splits[index_split]] + overall_points[index])/2\n",
    "\n",
    "           #increment\n",
    "            index_split += 1\n",
    "\n",
    "        #increment\n",
    "        index +=1\n",
    "\n",
    "        #reset this index to zero\n",
    "        index_split = 0\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns dictionary that had compiled points based on product's category\n",
    "\n",
    "def category_testing(df):\n",
    "    index = 0\n",
    "    index_split=0\n",
    "    dictionary_test = {}\n",
    "\n",
    "    while index < len(df.index):\n",
    "        #within each categories, there are different genres\n",
    "        categories = df['categories'].iloc[index]\n",
    "\n",
    "        #split each categories\n",
    "        list_splits = categories.split(\",\")\n",
    "\n",
    "        #go through each category\n",
    "        while index_split < len(list_splits):\n",
    "\n",
    "            #strip to make it look cleaner\n",
    "            list_splits[index_split]= list_splits[index_split].strip(\"]\")\n",
    "            list_splits[index_split]= list_splits[index_split].strip(\"[\")\n",
    "            list_splits[index_split]= list_splits[index_split].strip()\n",
    "\n",
    "            if df['amazon-id'][index] not in dictionary_test:\n",
    "                dictionary_test[df['amazon-id'][index]] = dictionary[list_splits[index_split]]\n",
    "            else:\n",
    "                dictionary_test[df['amazon-id'][index]] = (dictionary_test[df['amazon-id'][index]] + dictionary[list_splits[index_split]])/2\n",
    "\n",
    "            index_split+=1\n",
    "\n",
    "        index_split = 0\n",
    "        index +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root genre: \n",
    "\n",
    "Creating a dictionary of average star rating per root-genre based on training data. Then using dictionary to predict star rating of test data. \n",
    "\n",
    "Similarly to Categories, we did not believe that is a good predictor, especially when used alone and not in conjunction with other features with more predictive powers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary based on genre and the average star rating it has\n",
    "\n",
    "def root_genre_rating(df):\n",
    "    genre_list={}\n",
    "    index_genre =0\n",
    "    \n",
    "    root_genre = df['root-genre']\n",
    "\n",
    "    #going through each categories\n",
    "    while index_genre < len(df.index):\n",
    "\n",
    "            #if genre is not within the category, add it to the dictionary\n",
    "        if root_genre[index_genre] not in genre_list.keys():\n",
    "            genre_list[root_genre[index_genre]] = overall_points[index_genre]\n",
    "\n",
    "            #if genre is within the category, add the value with the old value and average it\n",
    "        else:\n",
    "            genre_list[root_genre[index_genre]] = (genre_list[root_genre[index_genre]] + overall_points[index_genre])/2\n",
    "\n",
    "           #increment\n",
    "        index_genre += 1\n",
    "    return genre_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_genre_testing(df):\n",
    "    index = 0\n",
    "    genre_test = {}\n",
    "\n",
    "    while index < len(df.index):\n",
    "\n",
    "        if df['amazon-id'][index] not in genre_test:\n",
    "            genre_test[df['amazon-id'][index]] = genre_list[df['root-genre'][index]]\n",
    "        else:\n",
    "            genre_test[df['amazon-id'][index]] = (genre_test[df['amazon-id'][index]] + genre_list[df['root-genre'][index]])/2\n",
    "\n",
    "        index +=1\n",
    "    return genre_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SalesRank:\n",
    "\n",
    "We believed that salesRank would be a good indicator of customer preferences and incorporating this feature would help the model predict better - a product with a high sales rank would be more likely to be rated as \"Awesome\".\n",
    "\n",
    "As with other omitted features, we believed that it would not be a good predictor on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "shift_salesRank: The minimum salesRank in the dataset was 6. We wanted to make sure it started at 0 - this method\n",
    "normalizes salesRank.\n",
    "* inputs: None\n",
    "* outputs: Returns new train dataset with salesRank normalized\n",
    "\"\"\"\n",
    "def shift_salesRank():\n",
    "    print(\"Current Minimum SalesRank: \", train_data['salesRank'].min(), \"Current Maximum SalesRank: \", train_data['salesRank'].max())\n",
    "    \n",
    "    arr = pd.factorize(train_data['salesRank'])\n",
    "    \n",
    "    train_new = train_data.assign(salesRank= arr[0])\n",
    "    \n",
    "    print(\"Current Minimum SalesRank: \", train_new['salesRank'].min(), \"Current Maximum SalesRank: \", train_new['salesRank'].max())\n",
    "    \n",
    "    return train_new "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related:\n",
    "    \n",
    "Related showed how connected products were to other products in the dataset. We aimed to perform network analysis on this column so that we could give more 'connected' reviews importance by weighting it more. \n",
    "\n",
    "Parsing the text was difficult and we settled on getting counts of also bought, bought together and bought after viewing keys for each observation. The idea was to gauge how connected products were based on the counts of these keys. However, the f1 score for the model used (Logistic Regression) was unsatisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "trim_curly: All entries in the related column start and end with curly braces parses it out from all strings in 'related'\n",
    "* inputs: None\n",
    "* outputs: Returns a list containing all strings in 'related'.\n",
    "\"\"\"\n",
    "\n",
    "def trim_curly():\n",
    "    \n",
    "    col_le = len(train_data['related'])\n",
    "    i = 0\n",
    "    l = []\n",
    "    \n",
    "    while (i < col_le):\n",
    "        l.append(train_data['related'][i].replace('{', '').replace('}', ''))\n",
    "        i= i+1\n",
    "    \n",
    "    return l\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "related_dicts: Performs get_dict across all strings in 'related'.\n",
    "* inputs: l: a list - or the related column.\n",
    "* outputs: d: a list of dictionaries\n",
    "\"\"\"\n",
    "def related_dicts(l):\n",
    "    res=[]\n",
    "    \n",
    "    for i in l:\n",
    "        d= get_dict(i)\n",
    "        res.append(d)\n",
    "    \n",
    "    return res\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods below obtain counts for the three keys in the 'related' column for each row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "also_bought_c: Getting count of also bought key from a single observation.\n",
    "* inputs: d: The dictionary obtained after converting a single string in 'related'.\n",
    "* outputs: count: count of values mapped to the also bought key.\n",
    "\"\"\"\n",
    "def also_bought_c(d):\n",
    "    count=0\n",
    "    \n",
    "    if 'also_bought' in d:\n",
    "        count= len(d['also_bought'])\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "buy_after_viewing_c: Getting count of buy after viewing key from a single observation.\n",
    "* inputs: d: The dictionary obtained after converting a single string in related.\n",
    "* outputs: count: count of values mapped to the bought after key.\n",
    "\"\"\"\n",
    "def buy_after_viewing_c(d):\n",
    "    count=0\n",
    "    \n",
    "    if 'buy_after_viewing' in d:\n",
    "        count= len(d['buy_after_viewing'])\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bought_together_c: Getting count of bought together key from a single observation.\n",
    "* inputs: d: The dictionary obtained after converting a single string in 'related'.\n",
    "* outputs: count: count of values mapped to the bought together key.\n",
    "\"\"\"\n",
    "def bought_together_c(d):\n",
    "    count=0\n",
    "    \n",
    "    if 'bought_together' in d:\n",
    "        count= len(d['bought_together'])\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ab_tot: Getting count of also bought key from all observations in 'related.'\n",
    "* inputs: l: The list of dictionaries obtained by parsing 'related'.\n",
    "* outputs: count: count of values mapped to the bought after key.\n",
    "\"\"\"\n",
    "\n",
    "def ab_tot(l):\n",
    "    res=[]\n",
    "    \n",
    "    for i in l:\n",
    "        res.append(also_bought_c(i))\n",
    "        \n",
    "    return res\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bt_tot: Getting count of bought together key from all observations in 'related.'\n",
    "* inputs: l: The list of dictionaries obtained by parsing 'related'.\n",
    "* outputs: count: count of values mapped to the bought after key.\n",
    "\"\"\"\n",
    "\n",
    "def bt_tot(l):\n",
    "    res=[]\n",
    "    \n",
    "    for i in l:\n",
    "        res.append(bought_together_c(i))\n",
    "        \n",
    "    return res\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bav_tot: Getting count of buy after viewing key from all observations in 'related.'\n",
    "* inputs: l: The list of dictionaries obtained by parsing 'related'.\n",
    "* outputs: count: count of values mapped to the bought after key.\n",
    "\"\"\"\n",
    "def bav_tot(l):\n",
    "    res=[]\n",
    "    \n",
    "    for i in l:\n",
    "        res.append(buy_after_viewing_c(i))\n",
    "        \n",
    "    return res\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the failed model is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the training data\n",
    "\n",
    "train_new = shift_salesRank()\n",
    "l= trim_curly()\n",
    "res= related_dicts(l)\n",
    "\n",
    "ab_count= ab_tot(res)\n",
    "bt_count= bt_tot(res)\n",
    "bav_count= bav_c(res)\n",
    "train_data['ab-count']= ab_count\n",
    "train_data['bt-count']= bt_count\n",
    "train_data['bav-count']=bav_count\n",
    "\n",
    "is_awesome = lambda x: 1 if np.mean(x) > 4.5 else 0\n",
    "train_new = train_data.groupby('amazon-id').agg({'salesRank': 'mean', 'ab-count': 'mean', 'bt-count': 'mean','bav-count': 'mean','overall': is_awesome})\n",
    "X_train, y_train = train_data[['salesRank', 'ab-count', 'bt-count', 'bav-count']], train_data['overall']\n",
    "\n",
    "# Parsing the test data\n",
    "\n",
    "test_data = pd.read_csv('./data/Test.csv')\n",
    "ltest= trim_curly()\n",
    "res1= related_dicts(ltest)\n",
    "ab_count1= ab_tot(res1)\n",
    "bt_count1= bt_tot(res1)\n",
    "bav_count1= bav_c(res1)\n",
    "\n",
    "test_data['ab-count']= ab_count1\n",
    "test_data['bt-count']= bt_count1\n",
    "test_data['bav-count']=bav_count1\n",
    "\n",
    "test_new = test_data.groupby('amazon-id').agg({'salesRank': 'mean', 'ab-count': 'mean', 'bt-count': 'mean','bav-count': 'mean'})\n",
    "\n",
    "# Fit model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "preds = model.predict(X_test)\n",
    "output = pd.DataFrame({'amazon-id': X_test.index, 'Awesome': preds})\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "# The accuracy of the model for three cross validations:\n",
    "# array([0.71400027, 0.71403111, 0.71405039])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using bayesian optimization\n",
    "\n",
    "#space = {'loss': loss, 'C': cs, 'dual': dual, 'penalty': penalty}\n",
    "#from hyperopt import fmin, tpe\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set up objective function\n",
    "#def objective(params):\n",
    "#    params = {'loss': params['loss'], 'C': params['C'], 'dual': params['dual'], 'penalty': params['penalty']}\n",
    "#    svc_clf = LinearSVC(**params) \n",
    "#    best_score = cross_val_score(svc_clf, x_train, y_train, scoring='accuracy', cv=3, n_jobs=4).mean()\n",
    "#    loss = 1 - best_score\n",
    "#    return loss\n",
    "\n",
    "# Run the algorithm\n",
    "#best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)\n",
    "#print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
